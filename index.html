<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yue Chen (陈越)</title>
  
  <meta name="author" content="Yue Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⭐️</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yue Chen</name>
              </p>
              <p>
                Yue Chen is currently a graduate student at <a href="https://english.pku.edu.cn/">Peking University</a> with 
                Agibot Lab advised by Professor <a href="https://zsdonghao.github.io/">Hao Dong</a>.
              </p>
              <p>
                  My research interest is broadly in Robotics, 3D Computer Vision and large language models (LLMs), 
                  with particular interests in generalizable object manipulation. 
              </p>
              <p>
                Email: yuechen020614 [at] gmail.com 
              </p>
              <p style="text-align:center">
                <a href="mailto:yuechen020614@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=v8ehFSQAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Cold114514">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yuechen.JPG"><img style="width:80%;max-width:100%" alt="profile photo" src="images/yuechen.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>&nbsp&nbsp&nbsp&nbsp&nbsp (* denotes equal contribution)
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:30px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="dexgarmentlab_stop()" onmouseover="dexgarmentlab_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dexgarmentlab_image'>
                  <img src='images/dexgarmentlab.jpg' width="220"></div>
                <img src='images/dexgarmentlab.jpg' width="220">
              </div>
              <script type="text/javascript">
                function dexgarmentlab_start() {
                  document.getElementById('dexgarmentlab_image').style.opacity = "1";
                }

                function dexgarmentlab_stop() {
                  document.getElementById('dexgarmentlab_image').style.opacity = "0";
                }
                dexgarmentlab_stop()
              </script>
            </td>
            
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://et-seed.github.io/"> -->
              <papertitle> DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy
              </papertitle>
              </a>
              <br>
              Yuran Wang*,
              <a href="https://alwaysleepy.github.io/">Ruihai Wu</a>*,
              <strong>Yue Chen</strong>*,
              Jiarui Wang
              Jiaqi Liang,
              <a href="https://alwaysleepy.github.io/">Ziyu Zhu</a>,
              <a href="https://geng-haoran.github.io/">Haoran Geng</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>Under Review</em>
              <br>
              <!-- <a href="https://et-seed.github.io/">project page</a> -->
               project page (coming soon)
              /
              paper (coming soon)
              <!-- <a href="https://arxiv.org/abs/2411.03990">paper</a> -->
             <!-- paper (coming soon) -->
              /
              <!-- <a href="https://github.com/Cold114514/ET-SEED">code</a> -->
             code (coming soon)
              /
             video (coming soon)
              <!-- <a href="https://et-seed.github.io/static/videos/ET-SEED_sup_video.mp4">video</a> -->
              <p></p>
              <p> We introduce DexGarmentLab, a realistic sim environment for bimanual dexterous garment manipulation. Based on this environment, we propose a new benchmark, an efficient data collection pipeline, and a novel policy framework that uses category-level visual correspondences for few-shot garment manipulation. </p>
            </td>
          </tr>
          
          <tr onmouseout="trustrag_stop()" onmouseover="trustrag_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='trustrag_image'>
                  <img src='images/trustrag.jpg' width="220"></div>
                <img src='images/trustrag.jpg' width="220">
              </div>
              <script type="text/javascript">
                function trustrag_start() {
                  document.getElementById('trustrag_image').style.opacity = "1";
                }

                function trustrag_stop() {
                  document.getElementById('trustrag_image').style.opacity = "0";
                }
                trustrag_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://et-seed.github.io/"> -->
              <papertitle> TrustRAG: Enhancing Robustness and Trustworthiness in RAG
              </papertitle>
              </a>
              <br>
              Huichi Zhou*, 
              Kin-Hei Lee*, 
              Zhonghao Zhan*, 
              <strong>Yue Chen</strong>,
              Zhenhao Li, 
              Zhaoyang Wang,
              Hamed Haddadi, 
              Emine Yilmaz
              <br>
              <em>Under Review</em>
              <br> 
              <a href="https://trust-rag.github.io/">project page</a>
              /
              <a href="https://arxiv.org/pdf/2501.00879">paper</a>
              /
              <a href="https://github.com/HuichiZhou/TrustRAG">code</a>
              <p></p>
              <p> We introduce TrustRAG, a robust Retrieval-Augmented Generation (RAG) framework. It defends against corpus poisoning attacks by a two-stage mechanism: identifying potential attack patterns with K-means clustering and detecting malicious docs via self-assessment.  </p>
            </td>
          </tr>


          <tr onmouseout="garmentpile_stop()" onmouseover="garmentpile_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='garmentpile_image'>
                  <img src='images/garmentpile.jpg' width="190" align="margin-right"></div>
                <img src='images/garmentpile.jpg' width="190">
              </div>
              <script type="text/javascript">
                function garmentpile_start() {
                  document.getElementById('garmentpile_image').style.opacity = "1";
                }

                function garmentpile_stop() {
                  document.getElementById('garmentpile_image').style.opacity = "0";
                }
                garmentpile_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://et-seed.github.io/"> -->
              <papertitle> Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation
              </papertitle>
              </a>
              <br>
              <a href="https://warshallrho.github.io/">Ruihai Wu</a>*,
              <a href="https://alwaysleepy.github.io/">Ziyu Zhu</a>*,
              Yuran Wang*,
              <strong>Yue Chen</strong>,
              Jiarui Wang
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>CVPR 2025</em>
              <br> 
              <a href="https://garmentpile.github.io/">project page</a>
               <!-- project page (coming soon) -->
              /
              <!-- paper (coming soon) -->
              <a href="https://arxiv.org/abs/2503.09243">paper</a>
             <!-- paper (coming soon) -->
              /
              <a href="https://github.com/AlwaySleepy/Garment-Pile">code</a>
             <!-- code (coming soon) -->
              <!-- <a href="https://et-seed.github.io/static/videos/ET-SEED_sup_video.mp4">video</a> -->
              <p></p>
              <p> We study the novel task of cluttered garments manipulation using dense visual affordance, with generalization towards diverse states, and propose a novel adaptation module to reorganize cluttered garments into configurations conducive to manipulation. </p>
            </td>
          </tr>

          <tr onmouseout="etseed_stop()" onmouseover="etseed_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='etseed_image'>
                  <img src='images/etseed.png' width="187"></div>
                <img src='images/etseed.png' width="187">
              </div>
              <script type="text/javascript">
                function etseed_start() {
                  document.getElementById('etseed_image').style.opacity = "1";
                }

                function etseed_stop() {
                  document.getElementById('etseed_image').style.opacity = "0";
                }
                etseed_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://et-seed.github.io/"> -->
              <papertitle>ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy
              </papertitle>
              </a>
              <br>
              <a href="https://crtie.github.io/">Chenrui Tie</a>*,
              <strong>Yue Chen</strong>*,
              <a href="https://warshallrho.github.io/">Ruihai Wu</a>*,
              Boxuan Dong,
              Zeyi Li,
              <a href="https://chongkaigao.com/">Chongkai Gao</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICLR 2025</em>
              <br>
              <a href="https://et-seed.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2411.03990">paper</a>
<!--              paper (coming soon)-->
              /
              <a href="https://github.com/Cold114514/ET-SEED">code</a>
<!--              code (coming soon)-->
              /
<!--              video (coming soon)-->
              <a href="https://et-seed.github.io/static/videos/ET-SEED_sup_video.mp4">video</a>
              <p></p>
              <p>We theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level SE(3) equivariant diffusion policy in an end-to-end manner. </p>
            </td>
          </tr>


          <tr onmouseout="eqvafford_stop()" onmouseover="eqvafford_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='eqvafford_image'>
                  <img src='images/eqvafford.jpg' width="187"></div>
                <img src='images/eqvafford.jpg' width="187">
              </div>
              <script type="text/javascript">
                function eqvafford_start() {
                  document.getElementById('eqvafford_image').style.opacity = "1";
                }

                function eqvafford_stop() {
                  document.getElementById('eqvafford_image').style.opacity = "0";
                }
                eqvafford_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://warshallrho.github.io/unigarmentmanip/"> -->
              <papertitle>EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning
              </papertitle>
              </a>
              <br>
              <strong>Yue Chen</strong>*,
              <a href="https://crtie.github.io/">Chenrui Tie</a>*,
              <a href="https://warshallrho.github.io/">Ruihai Wu</a>*,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>CVPR 2024 Workshop EquiVision</em>
              <br>
              <!-- <a href="https://warshallrho.github.io/unigarmentmanip/">project page</a> -->
              <a href="https://arxiv.org/pdf/2408.01953">paper</a>
              <!-- <a href="https://www.youtube.com/watch?v=N5NYt-XJDOs">video</a> -->
              <p></p>
              <p>We propose EqvAfford framework, with novel designs to guarantee the SE(3) equivariance in point-level affordance learning for downstream robotic manipulation. </p>
            </td>
          </tr>

        <!-- </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Other Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="tensorlayer_stop()" onmouseover="tensorlayer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tensorlayer_image'>
                  <img src='images/tl_transparent_logo.png' width="187"></div>
                <img src='images/tl_transparent_logo.png' width="187">
              </div>
              <script type="text/javascript">
                function tensorlayer_start() {
                  document.getElementById('tensorlayer_image').style.opacity = "1";
                }

                function tensorlayer_stop() {
                  document.getElementById('tensorlayer_image').style.opacity = "0";
                }
                tensorlayer_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/tensorlayer/TensorLayer">
              <papertitle>TensorLayer
              </papertitle>
              </a>
              <br>
              <br>
              A deep learning and reinforcement learning library designed for researchers and engineers.
              <br>
              <a href="https://twitter.com/ImperialDSI/status/923928895325442049">ACM MM Best Open Source Software Award, <em>2017</em>. </a>
              <br>
              I am one of the main contributors of its 2.0 version.
              <br>
              <br>
              <a href="https://github.com/tensorlayer/TensorLayer">GitHub</a>
              /
              <a href="https://github.com/tensorlayer/TensorLayer/stargazers">star (7000+)</a>
              /
              <a href="https://github.com/tensorlayer/TensorLayer/forks">fork (1600+)</a>
              /
              <a href="https://github.com/tensorlayer/TensorLayer/graphs/contributors">contributors</a>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="spreadsheet_stop()" onmouseover="spreadsheet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='spreadsheet_image'>
                  <img src='images/spreadsheet.gif' width="187"></div>
                <img src='images/spreadsheet.gif' width="187">
              </div>
              <script type="text/javascript">
                function spreadsheet_start() {
                  document.getElementById('spreadsheet_image').style.opacity = "1";
                }

                function spreadsheet_stop() {
                  document.getElementById('spreadsheet_image').style.opacity = "0";
                }
                spreadsheet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.microsoft.com/en-us/research/project/spreadsheet-intelligence/">
              <papertitle>Spreadsheet Intelligence (Microsoft Research Asia)
              </papertitle>
              </a>
              <br>
              <br>
              Umbrella research project behind <a href="https://support.microsoft.com/en-us/office/analyze-data-in-excel-3223aab8-f543-4fda-85ed-76bb0295ffc4?ui=en-us&rs=en-us&ad=us">Ideas in Excel</a> of Microsoft Office 365 product.
              <br>
              Intelligent feature announced at Microsoft Ignite Conference and released on March, 2019.
              <br>
              Star of tomorrow excellent intern, <em>2019</em>.
              <br>
              <br>
              <a href="https://www.microsoft.com/en-us/research/project/spreadsheet-intelligence/">project page</a>
              <p></p>
            </td>
          </tr>
        </tbody></table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Reviewer Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              
              International Conference on Learning Representations (ICLR), 2025
              <br>
              International Conference on Machine Learning (ICML), 2025
              <br>
              AAAI Conference on Artificial Intelligence (AAAI), 2024, 2025

              <br>
              <!-- Volunteer: <a href="images/wine2020.pdf">WINE <em>2020</em></a> -->
            </td>
          </tr>
        </tbody></table>


        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
                Teaching Assistant:
            <br>
              &nbsp &nbsp &nbsp <a href="https://deep-generative-models.github.io/">Deep Generative Models</a>, <em> 2020, 2022 </em>
            <br>
                Guest Lecturer:
            <br>
              &nbsp &nbsp &nbsp Frontier Computing Research Practice (Course of Open Source Development), <em> 2024 </em>
            <br>
              &nbsp &nbsp &nbsp Introduction to Computing (Course of Dynamic Programming), <em> 2024 </em>
            </td>
          </tr>

        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Invited Talks</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Learning 3D Visual Representations for Robotic Manipulation,
              <br>
              &nbsp &nbsp &nbsp <em>National University of Singapore, Feb. 2025</em>
              <br>
              &nbsp &nbsp &nbsp <em>Beijing Normal University, Shanghai Tech University, TeleAI, Jan. 2025</em>
              <br>
              &nbsp &nbsp &nbsp <em>Tsinghua University, Johns Hopkins University, Carnegie Mellon University, Dec. 2024</em>
              <br>
              &nbsp &nbsp &nbsp <em>University of Hong Kong, Nov. 2024</em>
              <br>
              Unified Simulation, Benchmark and Manipulation for Garments, &nbsp&nbsp&nbsp  <em><a href="https://anysyn3d.github.io/about.html">AnySyn3D</a>, 2024</em>
              <br>
                &nbsp &nbsp &nbsp --- If you are interested in 3D Vision research, welcome to follow <a href="https://anysyn3d.github.io/about.html">AnySyn3D</a> that conducts various topics.
              <br>
              Visual Representations for Embodied Agent, &nbsp&nbsp&nbsp  <em>Chinese University of Hong Kong, Shenzhen, 2024</em>
              <br>
              <a href="http://www.csig3dv.net/2024/studentFotum.html">Visual Representations for Embodied Agent</a>, &nbsp&nbsp&nbsp  <em>China3DV, 2024</em>
              <br>
            </td>
          </tr>

        </tbody></table> -->


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Provincial Outstanding Graduates, &nbsp&nbsp&nbsp  <em>2024</em>
              <br>
              Sishiyanghua Medal (Only 10 in university) , &nbsp&nbsp&nbsp  <em> 2023</em>
              <br>
              National Scholarship, &nbsp&nbsp&nbsp  <em> 2022&2023</em>
              <br>
              National First Prize, China Undergraduate Mathematical Contest in Modeling, &nbsp&nbsp&nbsp  <em>2022</em>
              <br>
              CCPC & ACM-ICPC Regional Silver Medal(Guilin Site & Hangzhou Site & Shenyang Site), &nbsp&nbsp&nbsp  <em> 2022</em>
              <br>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                  <div style="float:right;">
                    Website template comes from <a href="https://jonbarron.info/">Jon Barron</a><br>
                      Last update: February, 2025
                  </div>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
